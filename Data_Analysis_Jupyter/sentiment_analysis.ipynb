{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis on Amazon Unlocked Phone Purchase Review data. Using Naive Bayes and LSTM to find the accuracy of sentiment analysis with training and testing models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from bs4 import BeautifulSoup  \n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "\n",
    "import logging\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Lambda\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, SimpleRNN, GRU\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from collections import defaultdict\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras import backend as K\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Brand Name</th>\n",
       "      <th>Price</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Review Votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>5</td>\n",
       "      <td>I feel so LUCKY to have found this used (phone...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>nice phone, nice up grade from my pantach revu...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>5</td>\n",
       "      <td>Very pleased</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>It works good but it goes slow sometimes but i...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>Great phone to replace my lost phone. The only...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>1</td>\n",
       "      <td>I already had a phone with problems... I know ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>2</td>\n",
       "      <td>The charging port was loose. I got that solder...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>2</td>\n",
       "      <td>Phone looks good but wouldn't stay charged, ha...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>5</td>\n",
       "      <td>I originally was using the Samsung S2 Galaxy f...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>3</td>\n",
       "      <td>It's battery life is great. It's very responsi...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Product Name Brand Name   Price  \\\n",
       "0  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "1  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "2  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "3  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "4  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "5  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "6  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "7  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "8  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "9  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "\n",
       "   Rating                                            Reviews  Review Votes  \n",
       "0       5  I feel so LUCKY to have found this used (phone...           1.0  \n",
       "1       4  nice phone, nice up grade from my pantach revu...           0.0  \n",
       "2       5                                       Very pleased           0.0  \n",
       "3       4  It works good but it goes slow sometimes but i...           0.0  \n",
       "4       4  Great phone to replace my lost phone. The only...           0.0  \n",
       "5       1  I already had a phone with problems... I know ...           1.0  \n",
       "6       2  The charging port was loose. I got that solder...           0.0  \n",
       "7       2  Phone looks good but wouldn't stay charged, ha...           0.0  \n",
       "8       5  I originally was using the Samsung S2 Galaxy f...           0.0  \n",
       "9       3  It's battery life is great. It's very responsi...           0.0  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load csv file\n",
    "df = pd.read_csv('Amazon_Unlocked_Mobile.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Product Name Brand Name    Price  \\\n",
      "134801  BLU Studio 5.0 C HD - Unlocked Cell Phones - R...        BLU  2000.00   \n",
      "123493                         Blu LIFE 8 Unlocked (Pink)        BLU   199.98   \n",
      "335592  Samsung Galaxy S Duos II S7582 DUAL SIM Factor...    Samsung   299.99   \n",
      "246353  Motorola Droid 2 A955 Verizon Phone 5MP Cam, W...   Motorola    82.00   \n",
      "273324  Nokia Lumia 920 32GB Unlocked GSM 4G LTE Windo...      Nokia   149.35   \n",
      "\n",
      "        Rating                                            Reviews  \\\n",
      "134801       5  For the price I paid for this devices, its fan...   \n",
      "123493       5  love love love it....good buy...recommend to a...   \n",
      "335592       4                                               Good   \n",
      "246353       1  Not good. Returned first phone and they sent m...   \n",
      "273324       4  Met expectations! I'm very satisfied!Even arri...   \n",
      "\n",
      "        Review Votes  Sentiment  \n",
      "134801           0.0          1  \n",
      "123493           0.0          1  \n",
      "335592           0.0          1  \n",
      "246353           0.0          0  \n",
      "273324           1.0          1  \n",
      "Load 27799 training examples and 3089 validation examples. \n",
      "\n",
      "Show a review in the training set : \n",
      " good product and fast shipping. thank you.\n"
     ]
    }
   ],
   "source": [
    "df.dropna(inplace=True) #drop null \n",
    "df = df[df['Rating'] != 3] #drop neutral rating\n",
    "\n",
    "#encode 4,5 as 1 for positive sentiment & 1,2 as 0 for negative sentiment\n",
    "df['Sentiment'] = np.where(df['Rating'] > 3, 1, 0)\n",
    "print(df.head())\n",
    "\n",
    "#split training and test \n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Reviews'], df['Sentiment'], \\\n",
    "                                                    test_size=0.1, random_state=0)\n",
    "\n",
    "print('Load %d training examples and %d validation examples. \\n' %(X_train.shape[0],X_test.shape[0]))\n",
    "print('Show a review in the training set : \\n', X_train.iloc[10])\n",
    "\n",
    "#sample less from positive for testing \n",
    "#for training repeat negative ones and put them twice and use half of the positive "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bag of words\n",
    "#### 1. find a word embedding to convert a text into a numerical representation. \n",
    "#### 2. fit the numerical representations of text to machine learning algorithms or deep learning architectures.\n",
    "\n",
    "1. preprocess raw & create clean reviews\n",
    "2. create bag of words using count vectorizor\n",
    "3. get feature vectors for each review\n",
    "4. fit feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edden/opt/anaconda3/lib/python3.7/site-packages/bs4/__init__.py:357: UserWarning: \"http://www.amazon.com/gp/product/B013YDFH3Y?redirect=true&ref_=cm_cr_ryp_prd_ttl_sol_0\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/edden/opt/anaconda3/lib/python3.7/site-packages/bs4/__init__.py:357: UserWarning: \"https://www.amazon.com/dp/B00K15KRV6/ref=cm_cr_ryp_prd_ttl_sol_22\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/edden/opt/anaconda3/lib/python3.7/site-packages/bs4/__init__.py:357: UserWarning: \"http://www.amazon.com/gp/product/B0193D539M?redirect=true&ref_=cm_cr_ryp_prd_ttl_sol_0\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show a cleaned review in the training set : \n",
      " good product and fast shipping thank you\n"
     ]
    }
   ],
   "source": [
    "#cleaning done is: remove html tag, remove special char & num, make lowercase, remove stop words, stemming\n",
    "def cleanText(raw_text, remove_stopwords=False, stemming=False, split_text=False, \\):\n",
    "    text = BeautifulSoup(raw_text, 'lxml').get_text() \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", text) \n",
    "    words = letters_only.lower().split()\n",
    "    if remove_stopwords: \n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    if stemming==True:\n",
    "        stemmer = SnowballStemmer('english') \n",
    "        words = [stemmer.stem(w) for w in words]\n",
    "    if split_text==True: \n",
    "        return (words)\n",
    "    return( \" \".join(words)) \n",
    "\n",
    "X_train_cleaned = []\n",
    "X_test_cleaned = []\n",
    "for d in X_train:\n",
    "    X_train_cleaned.append(cleanText(d))\n",
    "print('Show a cleaned review in the training set : ',  X_train_cleaned[10])\n",
    "for d in X_test:\n",
    "    X_test_cleaned.append(cleanText(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features : 19607 \n",
      "\n",
      "Show some feature names : \n",
      " ['aa', 'areable', 'boot', 'clean', 'crushing', 'distortions', 'excatly', 'frills', 'heart', 'inverter', 'lolit', 'movie', 'over', 'predictable', 'reconnecting', 'scaling', 'soldto', 'tapped', 'ubuntu', 'wedges']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit & transform  training data to doc-term matrix withwo CountVectorizer\n",
    "countVect = CountVectorizer() \n",
    "X_train_countVect = countVect.fit_transform(X_train_cleaned)\n",
    "print(\"Number of features : %d \\n\" %len(countVect.get_feature_names())) # there are 6378 \n",
    "print(\"Show some feature names : \\n\", countVect.get_feature_names()[::1000])\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train_countVect, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on validation set: 0.9184\n",
      "\n",
      "AUC score : 0.8790\n",
      "\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.80      0.83       778\n",
      "           1       0.93      0.96      0.95      2311\n",
      "\n",
      "    accuracy                           0.92      3089\n",
      "   macro avg       0.90      0.88      0.89      3089\n",
      "weighted avg       0.92      0.92      0.92      3089\n",
      "\n",
      "\n",
      "Confusion Matrix : \n",
      " [[ 622  156]\n",
      " [  96 2215]]\n"
     ]
    }
   ],
   "source": [
    "#print model eval predicted res\n",
    "def modelEvaluation(predictions):\n",
    "    print (\"\\nAccuracy on validation set: {:.4f}\".format(accuracy_score(y_test, predictions)))\n",
    "    print(\"\\nAUC score : {:.4f}\".format(roc_auc_score(y_test, predictions)))\n",
    "    print(\"\\nClassification report : \\n\", metrics.classification_report(y_test, predictions))\n",
    "    print(\"\\nConfusion Matrix : \\n\", metrics.confusion_matrix(y_test, predictions))\n",
    "predictions = mnb.predict(countVect.transform(X_test_cleaned))\n",
    "modelEvaluation(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    " - Train Word2Vec model using gensim library\n",
    " - Fit the feature vectors of the reviews to Random Forest Classifier\n",
    " ##### Here's the workflow of this part.\n",
    "     1. parse review to sentences bc Word2Vec model takes a list of sentences as input\n",
    "     \n",
    "     2. create vocab using Word2Vec model\n",
    "     \n",
    "     3. transform review into numerical representation by computing average feature vectors of words\n",
    "     \n",
    "     4. fit the average feature vectors to Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. parse into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27768 parsed sentence in the training set\n",
      "\n",
      "Show a parsed sentence in the training set : \n",
      " ['good', 'product', 'and', 'fast', 'shipping', 'thank', 'you']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "def parseSent(review, tokenizer, remove_stopwords=False):\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(cleanText(raw_sentence, remove_stopwords, split_text=True))\n",
    "    return sentences\n",
    "#parse each review in set into sentences\n",
    "sentences = []\n",
    "for review in X_train_cleaned:\n",
    "    sentences += parseSent(review, tokenizer)\n",
    "print('%d parsed sentence in the training set\\n'  %len(sentences))\n",
    "print('Show a parsed sentence in the training set : \\n',  sentences[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. create vocab using Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec model ...\n",
      "\n",
      "Number of words in the vocabulary list : 3254 \n",
      "\n",
      "Show first 10 words in the vocalbulary list  vocabulary list: \n",
      " ['the', 'i', 'it', 'and', 'phone', 'a', 'to', 'is', 'this', 'for']\n"
     ]
    }
   ],
   "source": [
    "num_features = 400                   \n",
    "min_word_count = 15               \n",
    "num_workers = 4       \n",
    "context = 10                                                                                          \n",
    "downsampling = 1e-3 \n",
    "\n",
    "print(\"Training Word2Vec model ...\\n\")\n",
    "w2v = Word2Vec(sentences, workers=num_workers, size=num_features, min_count = min_word_count,window = context, sample = downsampling)\n",
    "w2v.init_sims(replace=True)\n",
    "w2v.save(\"w2v_model\")\n",
    "\n",
    "print(\"Number of words in the vocabulary list : %d \\n\" %len(w2v.wv.index2word)) #4016 \n",
    "print(\"Show first 10 words in the vocalbulary list  vocabulary list: \\n\", w2v.wv.index2word[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. averaging feature vectors\n",
    "For words appear in the volcabulary list, compute average feature vectors of each word. The average feature vector is the numerical represenation of the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeFeatureVec(review, model, num_features):\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    nwords = 0.\n",
    "    index2word_set = set(model.wv.index2word) #index2word is the volcabulary list of the Word2Vec model\n",
    "    isZeroVec = True\n",
    "    for word in review:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec, model[word])\n",
    "            isZeroVec = False\n",
    "    if isZeroVec == False:\n",
    "        featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    counter = 0\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    for review in reviews:\n",
    "        reviewFeatureVecs[counter] = makeFeatureVec(review, model,num_features)\n",
    "        counter = counter + 1\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edden/opt/anaconda3/lib/python3.7/site-packages/bs4/__init__.py:357: UserWarning: \"http://www.amazon.com/gp/product/B013YDFH3Y?redirect=true&ref_=cm_cr_ryp_prd_ttl_sol_0\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/edden/opt/anaconda3/lib/python3.7/site-packages/bs4/__init__.py:357: UserWarning: \"https://www.amazon.com/dp/B00K15KRV6/ref=cm_cr_ryp_prd_ttl_sol_22\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/edden/opt/anaconda3/lib/python3.7/site-packages/bs4/__init__.py:357: UserWarning: \"http://www.amazon.com/gp/product/B0193D539M?redirect=true&ref_=cm_cr_ryp_prd_ttl_sol_0\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/edden/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:15: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set : 27799 feature vectors with 300 dimensions\n",
      "Validation set : 3089 feature vectors with 300 dimensions\n"
     ]
    }
   ],
   "source": [
    "X_train_cleaned = []\n",
    "for review in X_train:\n",
    "    X_train_cleaned.append(cleanText(review, remove_stopwords=True, split_text=True))\n",
    "trainVector = getAvgFeatureVecs(X_train_cleaned, w2v, num_features)\n",
    "print(\"Training set : %d feature vectors with %d dimensions\" %trainVector.shape)\n",
    "X_test_cleaned = []\n",
    "for review in X_test:\n",
    "    X_test_cleaned.append(cleanText(review, remove_stopwords=True, split_text=True))\n",
    "testVector = getAvgFeatureVecs(X_test_cleaned, w2v, num_features)\n",
    "print(\"Validation set : %d feature vectors with %d dimensions\" %testVector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on validation set: 0.9249\n",
      "\n",
      "AUC score : 0.8897\n",
      "\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.82      0.85       778\n",
      "           1       0.94      0.96      0.95      2311\n",
      "\n",
      "    accuracy                           0.92      3089\n",
      "   macro avg       0.91      0.89      0.90      3089\n",
      "weighted avg       0.92      0.92      0.92      3089\n",
      "\n",
      "\n",
      "Confusion Matrix : \n",
      " [[ 637  141]\n",
      " [  91 2220]]\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(trainVector, y_train)\n",
    "predictions = rf.predict(testVector)\n",
    "modelEvaluation(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM - no Word2Vec embedding\n",
    "#### Recurrent Neural Networks (RNN), capable of learning long-term dependencies.\n",
    "- LSTM with Word2Vec embedding to classify the reviews into positive and negative sentiment using Keras libarary.\n",
    "    1. prepare X_train and X_test to 2D tensor\n",
    "    2. train a simple LSTM \n",
    "        --> (embeddign layer => LSTM layer => dense layer)\n",
    "    3. compile and fit the model using log loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edden/opt/anaconda3/lib/python3.7/site-packages/keras_preprocessing/text.py:178: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (27799, 100)\n",
      "X_test shape: (3089, 100)\n",
      "y_train shape: (27799, 2)\n",
      "y_test shape: (3089, 2)\n"
     ]
    }
   ],
   "source": [
    "top_words = 20000  #only consider top 20000 words in the corpus\n",
    "maxlen = 100 \n",
    "batch_size = 32\n",
    "nb_classes = 2\n",
    "nb_epoch = 3\n",
    "\n",
    "tokenizer = Tokenizer(nb_words=top_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_seq = sequence.pad_sequences(sequences_train, maxlen=maxlen)\n",
    "X_test_seq = sequence.pad_sequences(sequences_test, maxlen=maxlen)\n",
    "\n",
    "y_train_seq = np_utils.to_categorical(y_train, nb_classes)\n",
    "y_test_seq = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "print('X_train shape:', X_train_seq.shape)\n",
    "print('X_test shape:', X_test_seq.shape)\n",
    "print('y_train shape:', y_train_seq.shape)\n",
    "print('y_test shape:', y_test_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edden/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/edden/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, dropout=0.2, recurrent_dropout=0.2)`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 128)         2560000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 2,691,842\n",
      "Trainable params: 2,691,842\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edden/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n",
      "/Users/edden/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "27799/27799 [==============================] - 129s 5ms/step - loss: 0.2781 - accuracy: 0.8858\n",
      "Epoch 2/3\n",
      "27799/27799 [==============================] - 126s 5ms/step - loss: 0.1595 - accuracy: 0.9421\n",
      "Epoch 3/3\n",
      "27799/27799 [==============================] - 129s 5ms/step - loss: 0.1150 - accuracy: 0.9603\n",
      "3089/3089 [==============================] - 3s 946us/step\n",
      "Test loss : 0.1708\n",
      "Test accuracy : 0.9440\n"
     ]
    }
   ],
   "source": [
    "#construct LSTM\n",
    "model1 = Sequential()\n",
    "model1.add(Embedding(top_words, 128, dropout=0.2))\n",
    "model1.add(LSTM(128, dropout_W=0.2, dropout_U=0.2)) \n",
    "model1.add(Dense(nb_classes))\n",
    "model1.add(Activation('softmax'))\n",
    "model1.summary()\n",
    "\n",
    "#https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6be9a291375c why i used adam optimizer\n",
    "#https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/ choosing loss\n",
    "model1.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "model1.fit(X_train_seq, y_train_seq, batch_size=batch_size, nb_epoch=nb_epoch, verbose=1)\n",
    "\n",
    "score = model1.evaluate(X_test_seq, y_test_seq, batch_size=batch_size)\n",
    "print('Test loss : {:.4f}'.format(score[0]))\n",
    "print('Test accuracy : {:.4f}'.format(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of weight matrix in the embedding layer :  (20000, 128)\n",
      "Size of weight matrix in the hidden layer :  (128, 512)\n",
      "Size of weight matrix in the output layer :  (128, 2)\n"
     ]
    }
   ],
   "source": [
    "#weight matrix of embedding layer\n",
    "model1.layers[0].get_weights()[0] \n",
    "print(\"Size of weight matrix in the embedding layer : \", model1.layers[0].get_weights()[0].shape) \n",
    "\n",
    "#weight matrix of hidden layer\n",
    "print(\"Size of weight matrix in the hidden layer : \", model1.layers[1].get_weights()[0].shape)\n",
    "\n",
    "#weight matrix of output layer\n",
    "print(\"Size of weight matrix in the output layer : \", model1.layers[2].get_weights()[0].shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM with Word2Vec Embedding\n",
    "##### In the LSTM model constructed above, the embedding class in Keras does not take the semantic similarity of the words into account. The model assigns random weights to the embedding layer and learn the embeddings by minimizing the global error of the network.\n",
    "- Instead of using random weights, this will use the pretrained word embeddings to initialize the weight of an embedding layer. Use Word2Vec embedding trained in to  intialize the weights of embedding layer in LSTM.\n",
    "    1. Load pretrained word embedding model\n",
    "    2. Construct embedding layer using embedding matrix as weights\n",
    "    3. Train a LSTM with Word2Vec embedding (embeddign layer => LSTM layer => dense layer)\n",
    "    4. Compile and fit the model using log loss function and ADAM optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embedding matrix :  (3254, 400)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edden/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "#https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa used this to w2v embedding\n",
    "w2v = Word2Vec.load(\"w2v_model\")\n",
    "\n",
    "embedding_matrix = w2v.wv.syn0 \n",
    "print(\"Shape of embedding matrix : \", embedding_matrix.shape) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edden/opt/anaconda3/lib/python3.7/site-packages/keras_preprocessing/text.py:178: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (27799, 100)\n",
      "X_test shape: (3089, 100)\n",
      "y_train shape: (27799, 2)\n",
      "y_test shape: (3089, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edden/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:32: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, dropout=0.2, recurrent_dropout=0.2)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, None, 400)         1301600   \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 128)               270848    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 1,572,706\n",
      "Trainable params: 1,572,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edden/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:37: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "/Users/edden/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "27799/27799 [==============================] - 142s 5ms/step - loss: 0.2619 - accuracy: 0.8920\n",
      "Epoch 2/3\n",
      "27799/27799 [==============================] - 143s 5ms/step - loss: 0.1591 - accuracy: 0.9407\n",
      "Epoch 3/3\n",
      "27799/27799 [==============================] - 157s 6ms/step - loss: 0.1222 - accuracy: 0.9552\n",
      "3089/3089 [==============================] - 5s 2ms/step\n",
      "Test loss : 0.162\n",
      "Test accuracy : 0.941\n"
     ]
    }
   ],
   "source": [
    "top_words = embedding_matrix.shape[0]\n",
    "maxlen = 100 \n",
    "batch_size = 32\n",
    "nb_classes = 2\n",
    "nb_epoch = 3\n",
    "\n",
    "tokenizer = Tokenizer(nb_words=top_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_seq = sequence.pad_sequences(sequences_train, maxlen=maxlen)\n",
    "X_test_seq = sequence.pad_sequences(sequences_test, maxlen=maxlen)\n",
    "\n",
    "y_train_seq = np_utils.to_categorical(y_train, nb_classes)\n",
    "y_test_seq = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "print('X_train shape:', X_train_seq.shape) \n",
    "print('X_test shape:', X_test_seq.shape) \n",
    "print('y_train shape:', y_train_seq.shape)\n",
    "print('y_test shape:', y_test_seq.shape)\n",
    "\n",
    "\n",
    "#w2v embedding layer\n",
    "embedding_layer = Embedding(embedding_matrix.shape[0],embedding_matrix.shape[1],weights=[embedding_matrix])\n",
    "\n",
    "\n",
    "#LSTM with embedding layer\n",
    "model2 = Sequential()\n",
    "model2.add(embedding_layer)\n",
    "model2.add(LSTM(128, dropout_W=0.2, dropout_U=0.2)) \n",
    "model2.add(Dense(nb_classes))\n",
    "model2.add(Activation('softmax'))\n",
    "model2.summary()\n",
    "model2.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model2.fit(X_train_seq, y_train_seq, batch_size=batch_size, nb_epoch=nb_epoch, verbose=1)\n",
    "\n",
    "score = model2.evaluate(X_test_seq, y_test_seq, batch_size=batch_size)\n",
    "print('Test loss : {:.3f}'.format(score[0]))\n",
    "print('Test accuracy : {:.3f}'.format(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of weight matrix in the embedding layer :  (4016, 300)\n",
      "Size of weight matrix in the hidden layer :  (300, 512)\n",
      "Size of weight matrix in the output layer :  (128, 2)\n"
     ]
    }
   ],
   "source": [
    "#weight matrix of embedding layer\n",
    "print(\"Size of weight matrix in the embedding layer : \",model2.layers[0].get_weights()[0].shape) \n",
    "#weight matrix of hidden layer\n",
    "print(\"Size of weight matrix in the hidden layer : \",model2.layers[1].get_weights()[0].shape)\n",
    "\n",
    "#weight matrix of output layer\n",
    "print(\"Size of weight matrix in the output layer : \", model2.layers[2].get_weights()[0].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
